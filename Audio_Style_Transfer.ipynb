{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWyo0kh0zPJfHGqlCg+XVb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKOGBWSmvrOo"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import torchvision.transforms as transforms\n",
        "import copy\n",
        "import librosa\n",
        "import soundfile as sf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hyper parameter\n",
        "N_FFT = 2048;"
      ],
      "metadata": {
        "id": "JEos6oh7HGUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#stft공식을 적용하고 audio_data와 style_sr을 return 해주는 함수\n",
        "def read_audio_spectum(filename):\n",
        "\tx, sr = librosa.load(filename, duration = 58.05) # Duration=58.05 so as to make sizes convenient\n",
        "\tS = librosa.stft(x, N_FFT)\n",
        "\tp = np.angle(S)\n",
        "\tS = np.log1p(np.abs(S))  \n",
        "\treturn S, sr"
      ],
      "metadata": {
        "id": "GQBGVkRQTUWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "style_audio, style_sr = read_audio_spectum('./alpha.wav')\n",
        "content_audio, content_sr = read_audio_spectum('./beta.wav')\n",
        "if(style_audio.shape == content_audio.shape):\n",
        "  print('Sample Size Same!') \n",
        "  print('Shape : ', style_audio.shape)\n",
        "  # 가져온 오디오 데이터의 크기가 같은지 확인\n",
        "  # 만약 오디오 길이가 50초보다 짧으면 error 발생\n",
        "else:\n",
        "  print(\"Not same!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_rQ62pOu6H6",
        "outputId": "68ddb7ee-6ec4-446a-8d35-4bd016deb6fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Size Same!\n",
            "Shape :  (1025, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "style_audio = style_audio.reshape([1,1025,-1])\n",
        "content_audio = content_audio.reshape([1,1025,-1])\n",
        "#if style_audio type is tensor\n",
        "#style_audio = style_audio.unsqueeze(dim = 0)\n",
        "#content_audio = content_audio.unsqueeze(dim = 0)\n",
        "print(style_audio.shape)\n",
        "print(content_audio.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2US2_W9prIP",
        "outputId": "cc3ab6f3-2284-4805-a912-5d29a2c02070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1025, 256)\n",
            "(1, 1025, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#클래스 선언\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "\t\t#1d convolution을 사용하는 이유는 noise 때문에\n",
        "    self.cnn1 = nn.Conv1d(in_channels=1025, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "  def forward(self, x):\n",
        "\t\t\tout = self.cnn1(x)\n",
        "\t\t\tout = out.view(out.size(0),-1)\n",
        "\t\t\treturn out\n",
        "\t\t\n",
        "class GramMatrix(nn.Module): #선형대수학에서 이용되는 matrix\n",
        "\tdef forward(self, input): \n",
        "\t\ta, b, c = input.size()#input을 3차원 numpy로 받음\n",
        "\t\tfeatures = input.view(a * b, c)\n",
        "\t  #torch.mm은 두 matrix를 곱해줌\n",
        "\t\tG = torch.mm(features, features.t())\n",
        "\t\treturn G.div(a * b * c)\n",
        "\t\n",
        "class StyleLoss(nn.Module): \n",
        "\t#style loss pytorch\n",
        "\tdef __init__(self, target, weight):\n",
        "\t\tsuper(StyleLoss, self).__init__()\n",
        "\t\tself.target = target.detach() * weight \n",
        "\t\tself.weight = weight\n",
        "\t\tself.gram = GramMatrix()\n",
        "\t\tself.criterion = nn.MSELoss()\n",
        "\t\n",
        "\tdef forward(self, input):\n",
        "\t\tself.output = input.clone()\n",
        "\t\tself.G = self.gram(input)\n",
        "\t\tself.G.mul_(self.weight)\n",
        "\t\tself.loss = self.criterion(self.G, self.target)\n",
        "\t\treturn self.output\n",
        "\n",
        "\tdef backward(self,retain_graph=True):\n",
        "\t\tself.loss.backward(retain_graph=retain_graph)\n",
        "\t\treturn self.loss\n"
      ],
      "metadata": {
        "id": "xUMoSTMX5_tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "#get_style_model_ans_losses를 위한 parameters\n",
        "style_weight=2500 #????\n",
        "style_layer_default = 'conv_1'\n",
        "def get_style_model_and_losses(cnn, style_float,style_weight=style_weight,style_layer = style_layer_default): #STYLE WEIGHT\n",
        "  cnn = copy.deepcopy(cnn) #???? 왜 deep copy를 하는지 모르겠음\n",
        "  style_losses = []\n",
        "  #cnn이 nn.Sequential로 가정\n",
        "  #순차적으로 활성화되어야 하는 모듈에 새로운 nn.Sequential을 만듭니다.\n",
        "  model = nn.Sequential()  # the new Sequential module network\n",
        "  gram = GramMatrix()  # we need a gram module in order to compute style targets\n",
        "  if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    gram = gram.cuda()\n",
        "  name = 'conv_1'\n",
        "  model.add_module(name, cnn.cnn1) #name cnn.cnn1 의 child 모듈 생성\n",
        "  if name in style_layer:\n",
        "    target_feature = model(style_float).clone()\n",
        "    target_feature_gram = gram(target_feature)\n",
        "    style_loss = StyleLoss(target_feature_gram, style_weight)\n",
        "    model.add_module(\"style_loss_1\", style_loss)\n",
        "    style_losses.append(style_loss)\n",
        "  return model, style_losses"
      ],
      "metadata": {
        "id": "mME1RDqAvZEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = CNN()\n",
        "learning_rate_initial = 0.03\n",
        "num_steps=100\n",
        "if torch.cuda.is_available():\n",
        "\tcnn = cnn.cuda()"
      ],
      "metadata": {
        "id": "5fF0-xMzFsM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adam optimizer 사용\n",
        "def get_input_param_optimizer(input_float):\n",
        "\tinput_param = nn.Parameter(input_float.data)\n",
        "\t\t#optimizer = optim.Adagrad([input_param], lr=learning_rate_initial, lr_decay=0.0001,weight_decay=0)\n",
        "\toptimizer = optim.Adam([input_param], lr=learning_rate_initial, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
        "\treturn input_param, optimizer"
      ],
      "metadata": {
        "id": "ie9ED64WE4sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_style_transfer(cnn, style_float, input_float, num_steps=num_steps, style_weight=style_weight): #STYLE WEIGHT, NUM_STEPS\n",
        "  print('Building the style transfer model..')\n",
        "  #style_audio 에서 학습한 model을 들고온다\n",
        "  model, style_losses= get_style_model_and_losses(cnn, style_float, style_weight) #model과 style loss를 return 받고\n",
        "  input_param, optimizer = get_input_param_optimizer(input_float)# optimizer의 파라미터 설정\n",
        "  print('Optimizing..')\n",
        "  print(1)\n",
        "  run = [0]\n",
        "  #중간 결과 출력\n",
        "  while run[0] <= num_steps:\n",
        "    def closure():\n",
        "           \t# correct the values of updated input image\n",
        "      input_param.data.clamp_(0, 1)\n",
        "\t\t\t\t#학습 하는 과정\n",
        "      optimizer.zero_grad()\n",
        "      model(input_param)\n",
        "      style_score = 0\n",
        "\n",
        "      for sl in style_losses:\n",
        "\t\t\t\t\t#print('sl is ',sl,' style loss is ',style_score)\n",
        "        style_score += sl.backward()\n",
        "\n",
        "      run[0] += 1\n",
        "      if run[0] % 100 == 0:\n",
        "        print(\"run {}:\".format(run))\n",
        "        print('Style Loss : {:8f}'.format(style_score.data)) #CHANGE 4->8 \n",
        "        print()\n",
        "\n",
        "      return style_score\n",
        "\n",
        "\n",
        "  optimizer.step(closure)\n",
        "  input_param.data.clamp_(0, 1)\n",
        "  return input_param.data"
      ],
      "metadata": {
        "id": "eBta8XysFgOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "if torch.cuda.is_available():\n",
        "\tstyle_float = Variable((torch.from_numpy(style_audio)).cuda())\n",
        "\tcontent_float = Variable((torch.from_numpy(content_audio)).cuda())\t\n",
        "else:\n",
        "\tstyle_float = Variable(torch.from_numpy(style_audio))\n",
        "\tcontent_float = Variable(torch.from_numpy(content_audio))\n",
        "input_float = content_float.clone()\n",
        "#main함수에서 실행\n",
        "output = run_style_transfer(cnn, style_float, input_float)\n",
        "if torch.cuda.is_available():\n",
        "\toutput = output.cpu()\n",
        "#===========================\n",
        "#output = output.squeeze(0)\n",
        "output = output.squeeze(0)  # 차원 확장\n",
        "output = output.numpy()\n",
        "#run_style_transfer에 있는 get_style_losses에서\n",
        "#style을 학습한 cnn1을 학습\n",
        "#optimizer를 torch로 했기때문에 \n",
        "#output.numpy로 변환해준다\n",
        "N_FFT=2048\n",
        "a = np.zeros_like(output)\n",
        "a = np.exp(output) - 1\n",
        "\t#지수함수로 변경 왜???\n",
        "#======================\n",
        "#output으로 출력해주는 함수\n",
        "p = 2 * np.pi * np.random.random_sample(a.shape) - np.pi\n",
        "for i in tqdm(range(500)):\n",
        "\tS = a * np.exp(1j*p)\n",
        "\tx = librosa.istft(S)\n",
        "\tp = np.angle(librosa.stft(x, N_FFT))\n",
        "OUTPUT_FILENAME = 'output.wav'\n",
        "sf.write(OUTPUT_FILENAME, x, style_sr)\n",
        "print('DONE...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "3O4sUsBJFnuw",
        "outputId": "5079ed7a-3518-418c-86ed-3fe5b75b4448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the style transfer model..\n",
            "Optimizing..\n",
            "1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-2b1db70e0906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0minput_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_float\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#main함수에서 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_style_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-da3ed2026725>\u001b[0m in \u001b[0;36mrun_style_transfer\u001b[0;34m(cnn, style_float, input_float, num_steps, style_weight)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m#중간 결과 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0;31m# correct the values of updated input image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0minput_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}